{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMA867_Group Project_Python script",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEy3H76oBB-N"
      },
      "source": [
        "EDA Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv0GGLUlBF6s"
      },
      "source": [
        "#eda on the insurance clean csv file\n",
        "pip install sweetviz\n",
        "import pandas as pd\n",
        "df = pd.read_csv(r\"D:\\MMA867\\group project\\Insurance_Data.csv\")\n",
        "\n",
        "#import sweetviz\n",
        "import sweetviz as sv\n",
        "\n",
        "#analyze the dataset\n",
        "eda_report = sv.analyze(df)\n",
        "\n",
        "#display the report\n",
        "eda_report.show_html('EDA.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOA2WKPZBNq2"
      },
      "source": [
        "Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fasFqdEBukz"
      },
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(r\"D:\\MMA867\\group project\\Insurance_Data_Clean-1.csv\")\n",
        "\n",
        "#Replace Gender column with dummy variable (Male = 1, Female = 0)\n",
        "df[\"Gender\"].replace({\"Male\": \"1\", \"Female\": \"0\"}, inplace=True)\n",
        "\n",
        "#Replace Vehicle Damage column with dummy variable (Yes = 1, No = 0)\n",
        "df[\"Vehicle_Damage\"].replace({\"Yes\": \"1\", \"No\": \"0\"}, inplace=True)\n",
        "\n",
        "#Replace Vehicle age column with dummy variable (<1 year = 1, 1-2 year = 2, >2 years = 3)\n",
        "df[\"Vehicle_Age\"].replace({\"< 1 Year\":\"1\", \"1-2 Year\":\"2\", \"> 2 Years\":\"3\"}, inplace=True)\n",
        "\n",
        "#Find the range of Age column (Min = 20 years, Max = 85 years)\n",
        "print(\"Max and Min are\", df.Age.max(), df.Age.min())\n",
        "\n",
        "#Find the range of Vintage column (Min = 10 days, Max = 299 days)\n",
        "print(\"Max and Min are\", df.Vintage.max(), df.Vintage.min())\n",
        "\n",
        "#Create a new column Age_0-24years, Age_25-33years, Age_41-50years, Age_51-60years, Age_61-70years, Age_71-80years, Age_81-90years with dummy variables\n",
        "# create a list of our conditions\n",
        "\n",
        "condition1 = [\n",
        "    (df['Age'] >= 0) & (df['Age'] <= 24)\n",
        "]\n",
        "condition2 = [\n",
        "              (df['Age'] >= 25) & (df['Age'] <= 33)\n",
        "]\n",
        "condition3 = [\n",
        "              (df['Age'] >= 34) & (df['Age'] <= 45)\n",
        "]\n",
        "condition4 = [\n",
        "    (df['Age'] >= 46) & (df['Age'] <= 60)\n",
        "]\n",
        "condition5 = [\n",
        "              (df['Age'] >= 61) \n",
        "]\n",
        "    \n",
        "\n",
        "# create a list of the values we want to assign for each condition\n",
        "values = ['1']\n",
        "\n",
        "# create a new column and use np.select to assign values to it using our lists as arguments\n",
        "df['Age_0-24years'] = np.select(condition1, values)\n",
        "df['Age_25-33years'] = np.select(condition2, values)\n",
        "df['Age_34-45years'] = np.select(condition3, values)\n",
        "df['Age_46-60years'] = np.select(condition4, values)\n",
        "df['Age_above60years'] = np.select(condition5, values)\n",
        "\n",
        "\n",
        "#Create a new columns Vintage_0-100days, Vintage_101-200days, Vindays_201-300days with dummy variables\n",
        "# create a list of our conditions\n",
        "\n",
        "condition1 = [\n",
        "    (df['Vintage'] >= 0) & (df['Vintage'] <= 100)\n",
        "]\n",
        "condition2 = [\n",
        "              (df['Vintage'] >= 101) & (df['Vintage'] <= 200)\n",
        "]\n",
        "condition3 = [\n",
        "              (df['Vintage'] >= 201) & (df['Vintage'] <= 300)\n",
        "]\n",
        "    \n",
        "\n",
        "# create a list of the values we want to assign for each condition\n",
        "values = ['1']\n",
        "\n",
        "# create a new column and use np.select to assign values to it using our lists as arguments\n",
        "df['Vintage_0-100days'] = np.select(condition1, values)\n",
        "df['Vintage_101-200days'] = np.select(condition2, values)\n",
        "df['Vintage_201-300days'] = np.select(condition3, values)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "#Create new column 'Vehicle Age Damage' by concatenating string variables between Vehicle Age and Vehicle Damage\n",
        "df['Vehicle_Age_Damage'] = df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "         \n",
        "#Create new column 'previously Insured Vehicle Damage' by concatenating string variables between Previously Insured and Vehicle Damage\n",
        "df['Previously_Insured_Vehicle_Damage'] = df['Previously_Insured'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "#Create new column 'Age_Previously_insured' by concatenating string variables between Age bins and Previously Insured \n",
        "df['Age_0-24years_Previously_Insured'] = df['Age_0-24years'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Age_25-33years_Previously_Insured'] = df['Age_25-33years'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Age_34-45years_Previously_Insured'] = df['Age_34-45years'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Age_46-60years_Previously_Insured'] = df['Age_46-60years'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Age_above60years_Previously_Insured'] = df['Age_above60years'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "\n",
        "#Create new column 'Age_Vehicle Age_Vehicle Damage' by concatenating string variables between Age bins * Vehicle Age * Vehicle Damage \n",
        "df['Age_0-24years_Vehicle_Age_Vehicle_Damage'] = df['Age_0-24years'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Age_25-33years_Vehicle_Age_Vehicle_Damage'] = df['Age_25-33years'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Age_34-45years_Vehicle_Age_Vehicle_Damage'] = df['Age_34-45years'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Age_46-60years_Vehicle_Age_Vehicle_Damage'] = df['Age_46-60years'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Age_above60years_Vehicle_Age_Vehicle_Damage'] = df['Age_above60years'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "\n",
        "#Create new column 'Vintage_Previously_insured' by concatenating string variables between Vintage bins and Previously Insured \n",
        "df['Vintage_0-100days_Previously_Insured'] = df['Vintage_0-100days'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Vintage_101-200days_Previously_Insured'] = df['Vintage_101-200days'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "df['Vintage_201-300days_Previously_Insured'] = df['Vintage_201-300days'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "#Create new column 'Vintage_Vehicle Age_Vehicle Damage' by concatenating string variables between Vintage bins * Vehicle Age * Vehicle Damage \n",
        "df['Vintage0-100days_Vehicle_Age_Vehicle_Damage'] = df['Vintage_0-100days'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Vintage101-200days_Vehicle_Age_Vehicle_Damage'] = df['Vintage_101-200days'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "df['Vintage201-300days_Vehicle_Age_Vehicle_Damage'] = df['Vintage_201-300days'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "#Create new column 'Gender_Previously_insured' by concatenating string variables between gender and Previously Insured \n",
        "df['Gender_Previously_Insured'] = df['Gender'].map(str) + df['Previously_Insured'].map(str)\n",
        "\n",
        "#Create new column 'Gender_Vehicle Age_Vehicle Damage' by concatenating string variables between gender * Vehicle Age * Vehicle Damage \n",
        "df['Gender_Vehicle_Age_Vehicle_Damage'] = df['Gender'].map(str) + df['Vehicle_Age'].map(str) + df['Vehicle_Damage'].map(str)\n",
        "\n",
        "#Create new column 'Gender_Vintage' by concatenating string variables between gender * Vintage bins \n",
        "df['Gender_Vintage0-100days'] = df['Gender'].map(str) + df['Vintage_0-100days'].map(str)\n",
        "\n",
        "df['Gender_Vintage101-200days'] = df['Gender'].map(str) + df['Vintage_101-200days'].map(str)\n",
        "\n",
        "df['Gender_Vintage201-300days'] = df['Gender'].map(str) + df['Vintage_201-300days'].map(str)\n",
        "\n",
        "#Factorize all the interation feature columns\n",
        "cleanup_nums = {\"Vehicle_Age_Damage\":     {\"10\": 1, \"20\": 2, \"30\": 3, \"11\": 4, \"21\": 5, \"31\": 6},\n",
        "               \"Previously_Insured_Vehicle_Damage\": {\"00\": 1, \"01\": 2, \"10\": 3, \"11\": 4},\n",
        "               \"Age_0-24years_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Age_25-33years_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Age_34-45years_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Age_46-60years_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Age_above60years_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Age_0-24years_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Age_25-33years_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Age_34-45years_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Age_46-60years_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Age_above60years_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Vintage_0-100days_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Vintage_101-200days_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Vintage_201-300days_Previously_Insured\": {\"10\": 1, \"11\": 2, \"00\": 3, \"01\": 4},\n",
        "               \"Vintage0-100days_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Vintage101-200days_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Vintage201-300days_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Gender_Previously_Insured\": {\"11\": 1, \"10\": 2, \"01\": 3, \"00\": 4},\n",
        "               \"Gender_Vehicle_Age_Vehicle_Damage\": {\"110\": 1, \"120\": 2, \"130\": 3, \"111\": 4, \"121\": 5, \"131\": 6, \"010\": 7, \"020\": 8, \"030\": 9, \"011\": 10, \"021\": 11, \"031\": 12},\n",
        "               \"Gender_Vintage0-100days\": {\"11\": 1, \"01\": 2, \"10\": 3, \"00\": 4},\n",
        "               \"Gender_Vintage101-200days\": {\"11\": 1, \"01\": 2, \"10\": 3, \"00\": 4},\n",
        "               \"Gender_Vintage201-300days\": {\"11\": 1, \"01\": 2, \"10\": 3, \"00\": 4}}\n",
        "            \n",
        "df = df.replace(cleanup_nums)   \n",
        "df.dtypes\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "#write the dataset to csv file\n",
        "df.to_csv('Insurance_Data_with_Features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVIUdYaOBwAo"
      },
      "source": [
        "Data Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZixTqux7CDIH"
      },
      "source": [
        "# data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data viz\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "\n",
        "# customize print\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.max_rows', 300)\n",
        "pd.set_option('display.float_format', lambda x: '%.8f' % x)\n",
        "\n",
        "from sklearn.metrics import classification_report,accuracy_score, roc_curve, roc_auc_score,confusion_matrix, precision_recall_curve,recall_score, precision_score, f1_score,auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFPbIYCbCEZi"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RelJZufNCK7O"
      },
      "source": [
        "path = \"C:/Users/jackz/Desktop/Siyang's School/Learning Courses/5_MMA867_Predictive Modeling/Assignment/course project/\"\n",
        "data = pd.read_csv(path+\"data/Insurance_Data_with_Features_final.csv\")\n",
        "data.info()\n",
        "# drop duplicate id columns\n",
        "data.drop(columns=['Unnamed: 0','Unnamed: 0.1','id'],axis=1,inplace=True)\n",
        "\n",
        "# no missing values\n",
        "data.isna().sum()\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_G3pJuCeEs"
      },
      "source": [
        "Data check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diy4wxtoCf5l"
      },
      "source": [
        "# data is highly imbalanced: 88% vs 12%\n",
        "sns.countplot(data['Response'],data=data)\n",
        "\n",
        "imb_class = data.Response.value_counts().reset_index()\n",
        "class_perc = data.Response.value_counts(normalize=True).reset_index()\n",
        "imb_class = imb_class.merge(class_perc,on='index')\n",
        "imb_class\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(data.corr(), annot = True, cmap = 'coolwarm', linewidths=3,center = 0)\n",
        "plt.title(\"Pearson correlation of Features\", y=1.05, size=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sj4qGV2CqNV"
      },
      "source": [
        "Model 1: LightGBM with Optuna- gbdt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWSs4gHrCsTs"
      },
      "source": [
        "#Data Preprocessing\n",
        "X = data[data.columns[~data.columns.isin(['Response'])]]\n",
        "y = data['Response']\n",
        "\n",
        "print(X.shape)\n",
        "X.head(3)\n",
        "\n",
        "##############################\n",
        "# separate numeric variables and categorical variables\n",
        "##############################\n",
        "## (1) get variable names\n",
        "num_vars = ['Age','Annual_Premium','Vintage']\n",
        "cat_vars = [col for col in X.columns if col not in num_vars]\n",
        "\n",
        "## (2) get columns locations\n",
        "num_indexes = [X.columns.get_loc(n) for n in X.columns if n in num_vars] \n",
        "cat_indexes = [X.columns.get_loc(c) for c in X.columns if c in cat_vars]\n",
        "\n",
        "# audit\n",
        "cat_vars,num_indexes\n",
        "\n",
        "# make sure categorical variables are all non-negative because negative label will be interpreted as missing by lightgbm\n",
        "(X[cat_vars].values < 0).any()   #Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).\n",
        "\n",
        "####################\n",
        "# Train test validation split \n",
        "####################\n",
        "## 70% training - 30% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.30 , random_state=123)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_train.sum()/y_train.count())\n",
        "print(y_valid.sum()/y_valid.count())\n",
        "\n",
        "#####################\n",
        "# Performance Evaluation Function - confusion matrix\n",
        "####################\n",
        "# https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "    ####################\n",
        "# Performance Evaluation Function - ROC Curve \n",
        "####################\n",
        "# code from : https://www.kaggle.com/vedbharti/classification-precision-recall-vs-roc-plot\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\",roc_auc_score(y_actual, y_pred_proba))\n",
        "\n",
        "    ####################\n",
        "# Performance Evaluation Function - PR Curve \n",
        "####################\n",
        "def plotpr(y_actual, y_pred_proba):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot(recall, precision)\n",
        "    plt.plot([0, 1], [0.123, 0.123], linestyle = '--')  #baseline is positive ratio\n",
        "    plt.xlabel('Recall', fontsize = 14)\n",
        "    plt.ylabel('Precision', fontsize = 14)\n",
        "    plt.title('Precision-Recall Curve', fontsize = 15)\n",
        "    print(\"AUCPR Score:\",auc(recall, precision))\n",
        "\n",
        "    ####################\n",
        "# Performance Evaluation Function - Profitability\n",
        "####################\n",
        "def net_profit(y_actual, y_pred_proba,clv=1320*5,acq_cost=1000,opp_cost=1320*5):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = confusion_matrix(y_actual, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkKh9509DnrI"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "# Approach 1 - Python API\n",
        "model = LGBMClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lgb_base = model.predict(X_valid)\n",
        "y_proba_lgb_base = model.predict_proba(X_valid)[:,1]\n",
        "y_pred_relabel_lgb_base = np.where(y_proba_lgb_base>0.13,1,0)\n",
        "\n",
        "plotroc(y_valid,y_proba_lgb_base)\n",
        "\n",
        "plotpr(y_valid,y_proba_lgb_base)\n",
        "\n",
        "# confusion matrix with 0.5 threshold\n",
        "evaluation(y_valid,y_pred_lgb_base)\n",
        "\n",
        "# confusion matrix with 0.13 threshold\n",
        "evaluation(y_valid,y_pred_relabel_lgb_base)\n",
        "\n",
        "lgb_np_base = net_profit(y_valid,y_proba_lgb_base)\n",
        "\n",
        "# net profit trend line\n",
        "sns.lineplot(data=lgb_np_base, x=\"threshold\", y=\"net_profit\")\n",
        "lgb_np_base[lgb_np_base['net_profit'] == lgb_np_base.net_profit.max()]\n",
        "\n",
        "#Hyperparameter tuning with Optuna\n",
        "import lightgbmX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.30 , random_state=123)\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_train.sum()/y_train.count())\n",
        "print(y_valid.sum()/y_valid.count())\n",
        "\n",
        "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html#lightgbm.Dataset.__init__\n",
        "dtrain = lightgbm.Dataset(X_train, label=y_train, categorical_feature=cat_indexes,free_raw_data=False)\n",
        "dvalid = lightgbm.Dataset(X_valid, label=y_valid, categorical_feature=cat_indexes,reference=dtrain)\n",
        "\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.__version__\n",
        "\n",
        "import optuna.integration.lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "params_1 = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        'is_unbalance': True,\n",
        "        'learning_rate': 0.03,\n",
        "        \"seed\": 123\n",
        "    }\n",
        "\n",
        "study_tuner = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Suppress information only outputs - otherwise optuna is quite verbose, which can be nice, but takes up a lot of space\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "\n",
        "# Run optuna LightGBMTunerCV tuning of LightGBM with cross-validation\n",
        "tuner = lgb.LightGBMTunerCV(params_1, \n",
        "                            dtrain, \n",
        "                            study=study_tuner,\n",
        "                            verbose_eval=False,                            \n",
        "                            early_stopping_rounds=70,\n",
        "                            num_boost_round=700,\n",
        "                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n",
        "                            seed = 123,\n",
        "                            folds=cv\n",
        "                           )\n",
        "\n",
        "tuner.run()\n",
        "\n",
        "print(tuner.best_params)\n",
        "# Classification error\n",
        "print(tuner.best_score)\n",
        "\n",
        "# Set-up a temporary set of best parameters that we will use as a starting point below.\n",
        "# Note that optuna will complain about values on the edge of the search space, so we move \n",
        "# such values a tiny little bit inside the search space.\n",
        "tmp_best_params = tuner.best_params\n",
        "if tmp_best_params['feature_fraction']==1:\n",
        "    tmp_best_params['feature_fraction']=1.0-1e-9\n",
        "if tmp_best_params['feature_fraction']==0:\n",
        "    tmp_best_params['feature_fraction']=1e-9\n",
        "if tmp_best_params['bagging_fraction']==1:\n",
        "    tmp_best_params['bagging_fraction']=1.0-1e-9\n",
        "if tmp_best_params['bagging_fraction']==0:\n",
        "    tmp_best_params['bagging_fraction']=1e-9  #tmp_best_params\n",
        "temp_best_params={'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'boosting_type': 'gbdt', 'is_unbalance': True, 'learning_rate': 0.03, 'seed': 123, 'feature_pre_filter': False, 'lambda_l1': 0.003897960139435608, 'lambda_l2': 9.416861984161165e-08, 'num_leaves': 40, 'feature_fraction': 0.5479999999999999, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 100}\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "########################\n",
        "# Declare an objective function - how we evaluate how good a set of hyperparameters are\n",
        "########################\n",
        "#https://optuna.readthedocs.io/en/stable/reference/distributions.html\n",
        "\n",
        "def objective(trial):\n",
        "    \n",
        "    # a) Specify a search space using distributions across plausible values of hyperparameters.\n",
        "    param = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\",\n",
        "        'is_unbalance':True,\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\":\"gbdt\",\n",
        "        'feature_pre_filter': False,\n",
        "        #\"boosting_type\": trial.suggest_categorical(\"boosting\", [\"gbdt\", \"dart\"]),               \n",
        "        #\"seed\": 123,\n",
        "        \"learning_rate\": trial.suggest_uniform('learning_rate',0.005,0.35),\n",
        "        'max_depth': trial.suggest_int('max_depth',1,15),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 500),\n",
        "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.35, 0.95),\n",
        "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.35, 0.95),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
        "        'lambda_l1': trial.suggest_uniform('lambda_l1', 1e-5, 10.0),\n",
        "        'lambda_l2': trial.suggest_uniform('lambda_l2', 1e-5, 10.0),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 6, 100)\n",
        "    }\n",
        "    \n",
        "    #if param[\"boosting_type\"] == \"dart\":\n",
        "    #    param[\"drop_rate\"] = trial.suggest_loguniform(\"drop_rate\", 1e-3, 1.0)\n",
        "    #    param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-3, 1.0)\n",
        "\n",
        "    \n",
        "    # b) Run LightGBM for the hyperparameter values and track performance on validation set\n",
        "    model = lgb.train(param,\n",
        "                      dtrain,\n",
        "                      num_boost_round = 2000,\n",
        "                      early_stopping_rounds = 100,\n",
        "                      valid_sets = [dtrain, dvalid],\n",
        "                      valid_names=['train','valid'],\n",
        "                     verbose_eval = 30\n",
        "                     )\n",
        "    score = model.best_score['valid']['auc']\n",
        "    return score\n",
        "\n",
        "    #lgbcv = lgb.cv(param,\n",
        "    #               dtrain,\n",
        "    #               folds=cv,\n",
        "    #               verbose_eval=False,                   \n",
        "    #               early_stopping_rounds=100,                   \n",
        "    #               num_boost_round=1000\n",
        "    #              )\n",
        "    \n",
        "    #cv_score = lgbcv['auc-mean'][-1] + lgbcv['auc-stdv'][-1]\n",
        "    #if cv_score<best_score:\n",
        "    #    training_rounds = len(list(lgbcv.values())[0] )\n",
        "    \n",
        "    # Return metric of interest\n",
        "    #return cv_score\n",
        "\n",
        "    # Suppress information only outputs - otherwise optuna is quite verbose, which can be nice, but takes up a lot of space\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "sampler = TPESampler()\n",
        "\n",
        "study = optuna.create_study(direction='maximize',sampler=sampler)  \n",
        "# warm starting point from LightGBMTunerCV\n",
        "study.enqueue_trial(temp_best_params)\n",
        "\n",
        "# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n",
        "# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n",
        "# by optuna or set neither timeout or n_trials so that we keep going until \n",
        "# the user interrupts (\"Cancel run\").\n",
        "study.optimize(objective,n_trials=100)\n",
        "\n",
        "print(study.best_params)\n",
        "\n",
        "# Best AUC on test set\n",
        "print(study.best_value)\n",
        "\n",
        "optuna.visualization.plot_optimization_history(study)\n",
        "optuna.visualization.plot_slice(study)\n",
        "optuna.visualization.plot_param_importances(study)\n",
        "\n",
        "best_params = {\n",
        "    \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\",\n",
        "        'is_unbalance':True,\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\":\"gbdt\",\n",
        "        'feature_pre_filter': False} \n",
        "best_params.update(study.best_params)\n",
        "best_params\n",
        "\n",
        "lgbfit = lgb.train(best_params,\n",
        "                   dtrain, \n",
        "                   num_boost_round = 2000,\n",
        "                   early_stopping_rounds = 100,\n",
        "                   valid_sets = [dtrain, dvalid],\n",
        "                   valid_names=['train','valid'])\n",
        "\n",
        "\n",
        "n_estimators = lgbfit.best_iteration\n",
        "n_estimators\n",
        "\n",
        "y_pred_proba_lgb_v3 = lgbfit.predict(X_valid,num_iteration=n_estimators)\n",
        "y_pred_label_lgb_v3 = np.where(y_pred_proba_lgb_v3>0.31,1,0)\n",
        "y_pred_proba_lgb_v3\n",
        "\n",
        "y_pred_proba_lgb_v3\n",
        "\n",
        "# confusion matrix with 0.13 threshold\n",
        "evaluation(y_valid,y_pred_label_lgb_v3)\n",
        "\n",
        "plotroc(y_valid,y_pred_proba_lgb_v3)\n",
        "plotpr(y_valid,y_pred_proba_lgb_v3)\n",
        "\n",
        "print('LGBM Base Accuracy : {}'.format(accuracy_score(y_valid,y_pred_label_lgb_v3)))\n",
        "print('LGBM Base ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,y_pred_proba_lgb_v3)))\n",
        "\n",
        "\n",
        "lgb_np_final = net_profit(y_valid,y_pred_proba_lgb_v3)\n",
        "\n",
        "# net profit trend line\n",
        "sns.lineplot(data=lgb_np_final, x=\"threshold\", y=\"net_profit\")\n",
        "lgb_np_final[lgb_np_final['net_profit'] == lgb_np_final.net_profit.max()]\n",
        "\n",
        "#Feature Importance:\n",
        "lgbfit.params\n",
        "\n",
        "import pickle\n",
        "# save best model to local disk\n",
        "filename = 'finalized_lightgbm_model.sav'\n",
        "pickle.dump(lgbfit, open(filename, 'wb'))\n",
        "\n",
        "#load the model from disk \n",
        "loaded_model = pickle.load(open(filename, 'rb')) result = loaded_model.score(X_test, Y_test) print(result)\n",
        "\n",
        "#Prediction on testing dataset\n",
        "#import test csv file\n",
        "data_test_orig = pd.read_csv(path+\"data/Insurance_test_with_Features.csv\")\n",
        "data_test_orig.info()\n",
        "# drop duplicate id columns\n",
        "X_test = data_test_orig.drop(columns=['Unnamed: 0','id'],axis=1)\n",
        "# make sure columns are matched as the lgbm defines categorical variables based on indexes\n",
        "print(X_train.columns)\n",
        "print(X_test.columns)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "X_train.head(2)\n",
        "X_test.head(2)\n",
        "y_pred_proba_lgb_test = lgbfit.predict(X_test,num_iteration=n_estimators)\n",
        "y_pred_label_lgb_test = np.where(y_pred_proba_lgb_test>0.31,1,0)\n",
        "y_pred_proba_lgb_test, y_pred_label_lgb_test\n",
        "y_pred_test = pd.DataFrame({'response_pred_probability':y_pred_proba_lgb_test,\n",
        "                            'response_pred_label_at_thresh0.31':y_pred_label_lgb_test},\n",
        "                           columns=['response_pred_probability','response_pred_label_at_thresh0.31'])\n",
        "y_pred_test.head()\n",
        "\n",
        "test_with_pred = pd.concat([data_test_orig,y_pred_test], axis=1)\n",
        "test_with_pred.head()\n",
        "test_with_pred['response_pred_label_at_thresh0.31'].value_counts()\n",
        "test_with_pred['response_pred_label_at_thresh0.31'].value_counts(normalize=True)\n",
        "test_with_pred['response_pred_decile'] = 10-pd.qcut(test_with_pred['response_pred_probability'], 10, labels=False)\n",
        "test_with_pred.groupby(['response_pred_decile'],as_index=False).agg({'response_pred_probability':{'min','max'},\n",
        "                                                                     'id':'count'})\n",
        "test_with_pred.to_csv('Insurance_Test_Prediction.csv', encoding='utf-8', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSDKZYCfHW83"
      },
      "source": [
        "Model 2: Linear Discriminant Analysis (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQyQpJfOISVe"
      },
      "source": [
        "# data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data viz\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "\n",
        "# customize print\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.max_rows', 300)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score,confusion_matrix, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "path = \"C:/Users/jackz/Desktop/Siyang's School/Learning Courses/5_MMA867_Predictive Modeling/Assignment/course project/\"\n",
        "data = pd.read_csv(path+\"data/Insurance_Data_with_Features_final.csv\")\n",
        "data.info()\n",
        "# drop duplicate id columns\n",
        "data.drop(columns=['Unnamed: 0','Unnamed: 0.1','id'],axis=1,inplace=True)\n",
        "data.head()\n",
        "data['Response'].value_counts()\n",
        "\n",
        "#Data Preprocessing\n",
        "####################\n",
        "# Train test split \n",
        "####################\n",
        "y = data['Response']\n",
        "cols = [x for x in data.columns if x not in ['Response', 'id']]\n",
        "X = data[cols]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=123)\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())\n",
        "X_train.shape\n",
        "\n",
        "# audit positive ratio \n",
        "print(y_train.sum()/y_train.count())\n",
        "print(y_test.sum()/y_test.count())\n",
        "\n",
        "###############\n",
        "# Oversampling\n",
        "###############\n",
        "X_oversampled, y_oversampled = resample(X_train[y_train == 0],\n",
        "                                        y_train[y_train == 0],\n",
        "                                        replace=True,\n",
        "                                        n_samples=X_train[y_train == 1].shape[0],\n",
        "                                        random_state=123)\n",
        "#\n",
        "# Append the oversampled minority class to training data and related labels\n",
        "#\n",
        "X_balanced = np.vstack((X_train[y_train == 1], X_oversampled))\n",
        "y_balanced = np.hstack((y_train[y_train == 1], y_oversampled))\n",
        "# perfectly balanced\n",
        "sum(y_balanced)/len(y_balanced)\n",
        "cols = X_test.columns.to_list()\n",
        "X_balanced_train = pd.DataFrame(X_balanced,columns=cols)\n",
        "\n",
        "####################\n",
        "# Standardize data as LDA requires data to follow Gaussian distribution\n",
        "####################\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "X_balanced_train=pd.DataFrame(scaler.fit_transform(X_balanced_train),columns=X_balanced_train.columns) # by default, standardscaler removes headers\n",
        "X_test=pd.DataFrame(scaler.transform(X_test),columns=X_test.columns)\n",
        "\n",
        "#####################\n",
        "# Performance Evaluation Function\n",
        "####################\n",
        "# Define model evalation functions - confusion matrix\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "####################\n",
        "# Performance Evaluation Function - ROC Curve \n",
        "####################\n",
        "# code from : https://www.kaggle.com/vedbharti/classification-precision-recall-vs-roc-plot\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\",roc_auc_score(y_actual, y_pred_proba))\n",
        "\n",
        "####################\n",
        "# Performance Evaluation Function - Profitability\n",
        "####################\n",
        "def net_profit(y_actual, y_pred_proba,clv=1320*5,acq_cost=1000,opp_cost=1320*5):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = confusion_matrix(y_actual, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "\n",
        "# If we want to use predictions\n",
        "# y_pred is an array of predictions\n",
        "def bestThresshold(y_true,y_pred):\n",
        "    best_thresh = None\n",
        "    best_score = 0\n",
        "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "        score = f1_score(y_true, np.array(y_pred)>thresh)\n",
        "        if score > best_score:\n",
        "            best_thresh = thresh\n",
        "            best_score = score\n",
        "    return best_score , best_thresh\n",
        "\n",
        "#LDA model\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "#Default LDA model without any tuning - base metric\n",
        "LDA_baseline = LinearDiscriminantAnalysis()\n",
        "LDA_baseline.fit(X_balanced_train, y_balanced)\n",
        "\n",
        "y_pred_proba_LDA_base =LDA_baseline.predict_proba(X_test)[:,1]\n",
        "best_score,best_thresh = bestThresshold(y_test,y_pred_proba_LDA_base)\n",
        "best_score,best_thresh\n",
        "y_pred_label_LDA_base = np.where(y_pred_proba_LDA_base>0.49,1,0)\n",
        "y_pred_label_LDA_base\n",
        "evaluation(y_test,y_pred_label_LDA_base)\n",
        "\n",
        "plotroc(y_test, y_pred_proba_LDA_base)\n",
        "lda_np_base = net_profit(y_actual=y_test, y_pred_proba=y_pred_proba_LDA_base, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=lda_np_base, x=\"threshold\", y=\"net_profit\")\n",
        "lda_np_base[lda_np_base['net_profit'] == lda_np_base.net_profit.max()]\n",
        "\n",
        "#Solver - lsqr or eigen\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
        "#######################\n",
        "### LDA - lsqr & eigen \n",
        "#######################\n",
        "LDA_t1 = LinearDiscriminantAnalysis(shrinkage='auto')  #automatic shrinkage using the Ledoit-Wolf lemma.\n",
        "params_t1 = {\n",
        "    'solver':('lsqr', 'eigen'),\n",
        "    'n_components': (1,3,5,7,9)     #Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction. If None, will be set to min(n_classes - 1, n_features). This parameter only affects the transform method.\n",
        "}\n",
        "\n",
        "gs_LDA_t1 = GridSearchCV(\n",
        "    estimator = LDA_t1,\n",
        "    param_grid = params_t1,\n",
        "    scoring = 'roc_auc',\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "LDA_t1_gs_results = gs_LDA_t1.fit(X_balanced_train, y_balanced)\n",
        "print('Mean ROC AUC: %f' % LDA_t1_gs_results.best_score_)\n",
        "print('Config: %s' % LDA_t1_gs_results.best_params_)\n",
        "\n",
        "LDA_t1_final = LinearDiscriminantAnalysis(**LDA_t1_gs_results.best_params_,\n",
        "                                          shrinkage='auto')\n",
        "LDA_t1_final.fit(X_balanced_train, y_balanced)\n",
        "\n",
        "y_pred_proba_LDA_t1 =LDA_t1_gs_results.predict_proba(X_test)[:,1]\n",
        "best_score2,best_thresh2 = bestThresshold(y_test,y_pred_proba_LDA_t1)\n",
        "best_score2,best_thresh2\n",
        "y_pred_label_LDA_t1 = np.where(y_pred_proba_LDA_t1>best_thresh2,1,0)\n",
        "y_pred_label_LDA_base\n",
        "evaluation(y_test,y_pred_label_LDA_t1)\n",
        "lda_np_2 = net_profit(y_actual=y_test, y_pred_proba=y_pred_proba_LDA_t1, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=lda_np_2, x=\"threshold\", y=\"net_profit\")\n",
        "lda_np_2[lda_np_2['net_profit'] == lda_np_2.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKPjyfDxKwWH"
      },
      "source": [
        "Model 3: LightGBM using dart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hluO65EK07I"
      },
      "source": [
        "pip install neptune-contrib\n",
        "pip install scikit-optimize\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import seaborn as sns # statistical data visualization\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, confusion_matrix, classification_report, accuracy_score # Measuring performance\n",
        "from sklearn.model_selection import train_test_split # splitting dataset\n",
        "import neptunecontrib.monitoring.skopt as sk_utils # optimizer\n",
        "import lightgbm as lgb # light gradient boosting\n",
        "import neptune # hyperparameters tunning\n",
        "import skopt # optimizer\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#data import\n",
        "data = pd.read_csv('Insurance_Data_with_Features_final.csv')\n",
        "data.head()\n",
        "\n",
        "data= data.drop(['Unnamed: 0',\t'Unnamed: 0.1', 'Driving_License'], axis=1)\n",
        "data.dtypes\n",
        "\n",
        "SEARCH_PARAMS = {'learning_rate': 0.4,\n",
        "                 'max_depth': 15,\n",
        "                 'num_leaves': 32,\n",
        "                 'feature_fraction': 0.8,\n",
        "                 'subsample': 0.2}\n",
        "FIXED_PARAMS={'objective': 'binary',\n",
        "              'metric': 'auc',\n",
        "              'is_unbalance':True,\n",
        "              'bagging_freq':5,\n",
        "              'boosting':'dart',\n",
        "              'num_boost_round':300,\n",
        "              'early_stopping_rounds':20}\n",
        "\n",
        "def train_evaluate(search_params): \n",
        "    y = data['Response']\n",
        "    cols = [x for x in data.columns if x not in ['Response', 'id']]\n",
        "    X = data[cols]\n",
        "    # separate numeric variables and categorical variables\n",
        "    ## (1) get variable names\n",
        "    num_vars = ['Age','Annual_Premium','Vintage']\n",
        "    cat_vars = [col for col in X.columns if col not in num_vars]\n",
        "    ## (2) get columns locations\n",
        "    num_indexes = [X.columns.get_loc(n) for n in X.columns if n in num_vars] \n",
        "    cat_indexes = [X.columns.get_loc(c) for c in X.columns if c in cat_vars]\n",
        "\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_indexes,free_raw_data=False)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=cat_indexes)\n",
        "    \n",
        "    params = {'metric':FIXED_PARAMS['metric'],\n",
        "              'objective':FIXED_PARAMS['objective'],\n",
        "              **search_params}\n",
        "\n",
        "    \n",
        "    model = lgb.train(params, train_data,\n",
        "                      valid_sets=[valid_data],\n",
        "                      num_boost_round=FIXED_PARAMS['num_boost_round'],\n",
        "                      early_stopping_rounds=FIXED_PARAMS['early_stopping_rounds'],\n",
        "                      valid_names=['valid'])\n",
        "\n",
        "    score = model.best_score['valid']['auc']\n",
        "    return score\n",
        "\n",
        "neptune.init(project_qualified_name='agnesagooly/lgbm',\n",
        "              api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2OGU5MTk2My01NGNiLTQ0ZGEtYTQyMi0xNGIwZmZlMGE4NjgifQ==')\n",
        "neptune.create_experiment('lgb-tuning',\n",
        "                               tags=['lgb-tuning', 'dart'],params=SEARCH_PARAMS)\n",
        "\n",
        "SPACE = [\n",
        "    skopt.space.Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),\n",
        "    skopt.space.Integer(1, 30, name='max_depth'),\n",
        "    skopt.space.Integer(10, 200, name='num_leaves'),\n",
        "    skopt.space.Real(0.1, 1.0, name='feature_fraction', prior='uniform'),\n",
        "    skopt.space.Real(0.1, 1.0, name='subsample', prior='uniform')\n",
        "]\n",
        "\n",
        "\n",
        "@skopt.utils.use_named_args(SPACE)\n",
        "def objective(**params):\n",
        "    return -1.0 * train_evaluate(params)\n",
        "\n",
        "\n",
        "monitor = sk_utils.NeptuneMonitor()\n",
        "results = skopt.forest_minimize(objective, SPACE, n_calls=100, n_random_starts=10, callback=[monitor])\n",
        "sk_utils.log_results(results)\n",
        "\n",
        "neptune.stop()\n",
        "\n",
        "# best score parameters\n",
        "# [('learning_rate', 0.01971835720802778), \n",
        "# ('max_depth', 1), \n",
        "# ('num_leaves', 90), \n",
        "# ('feature_fraction', 0.8054570258041184), \n",
        "# ('subsample', 0.6483251268967936)]\n",
        "y = data['Response']\n",
        "cols = [x for x in data.columns if x not in ['Response', 'id']]\n",
        "X = data[cols]\n",
        "# separate numeric variables and categorical variables\n",
        "## (1) get variable names\n",
        "num_vars = ['Age','Annual_Premium','Vintage']\n",
        "cat_vars = [col for col in X.columns if col not in num_vars]\n",
        "## (2) get columns locations\n",
        "num_indexes = [X.columns.get_loc(n) for n in X.columns if n in num_vars] \n",
        "cat_indexes = [X.columns.get_loc(c) for c in X.columns if c in cat_vars]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
        "\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_indexes,free_raw_data=False)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=cat_indexes)\n",
        "\n",
        "# retrain model using best parameters\n",
        "params = {'learning_rate': 0.01971835720802778,\n",
        "                  'max_depth': 1,\n",
        "                  'num_leaves': 90,\n",
        "                  'feature_fraction': 0.8054570258041184,\n",
        "                  'subsample': 0.6483251268967936,\n",
        "                  'objective': 'binary',\n",
        "                  'metric': 'auc',\n",
        "                  'is_unbalance':True,\n",
        "                  'bagging_freq':5,\n",
        "                  'boosting':'dart',\n",
        "                  'num_boost_round':300,\n",
        "                  'early_stopping_rounds':20}\n",
        "modelfit = lgb.train(params, train_data,\n",
        "                  valid_sets=[valid_data],\n",
        "                  num_boost_round=FIXED_PARAMS['num_boost_round'],\n",
        "                  early_stopping_rounds=FIXED_PARAMS['early_stopping_rounds'],\n",
        "                  valid_names=['valid'])\n",
        "\n",
        "n_estimators = modelfit.best_iteration\n",
        "n_estimators\n",
        "\n",
        "# If we want to use predictions\n",
        "# y_pred is an array of predictions\n",
        "def bestThresshold(y_true,y_pred):\n",
        "    best_thresh = None\n",
        "    best_score = 0\n",
        "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "        score = f1_score(y_true, np.array(y_pred)>thresh)\n",
        "        if score > best_score:\n",
        "            best_thresh = thresh\n",
        "            best_score = score\n",
        "    return best_score , best_thresh\n",
        "    \n",
        "y_pred_proba_fit = modelfit.predict(X_valid,num_iteration=n_estimators)\n",
        "bestThresshold(y_true=y_valid, y_pred=y_pred_proba_fit)\n",
        "y_pred_proba_fit = modelfit.predict(X_valid,num_iteration=n_estimators)\n",
        "y_pred_label= np.where(y_pred_proba_fit>0.38,1,0)\n",
        "y_pred_label\n",
        "# evaluation \n",
        "# for ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\",roc_auc_score(y_actual, y_pred_proba))\n",
        "    \n",
        "# for confusion metric and report\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "plotroc(y_valid,y_pred_proba_fit)\n",
        "print(evaluation(y_valid, y_pred_label))\n",
        "print('LGBM Accuracy : {}'.format(accuracy_score(y_valid,y_pred_label)))\n",
        "print('LGBM ROC_AUC_SCORE: {}'.format(roc_auc_score(y_valid,y_pred_proba_fit)))\n",
        "# Performance Evaluation Function - Profitability\n",
        "def net_profit(y_test, y_pred_proba,clv=1320*10, acq_cost=1000, opp_cost=1320*10):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = confusion_matrix(y_test, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "\n",
        "lgbm_profit = net_profit(y_test=y_valid, y_pred_proba=y_pred_proba_fit, clv=1320*10, acq_cost=200, opp_cost=0)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=lgbm_profit, x=\"threshold\", y=\"net_profit\")\n",
        "lgbm_profit[lgbm_profit['net_profit'] == lgbm_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B37NP_xMWyA"
      },
      "source": [
        "Model 4: Ada boost model with balanced data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g_Tz9KBMeQ3"
      },
      "source": [
        "pip install imbalanced-learn\n",
        "\n",
        "# Load libraries\n",
        "import imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score,confusion_matrix, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "#data import\n",
        "data = pd.read_csv(\"Insurance_Data_with_Features_final.csv\")\n",
        "data.columns\n",
        "data.drop(columns= ['Unnamed: 0', 'Unnamed: 0.1'],axis=1,inplace=True)\n",
        "data.head()\n",
        "data['Response'].value_counts()\n",
        "y = data['Response']\n",
        "cols = [x for x in data.columns if x not in ['Response', 'id']]\n",
        "X = data[cols]\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())\n",
        "\n",
        "X_oversampled, y_oversampled = resample(X_train[y_train == 0],\n",
        "                                        y_train[y_train == 0],\n",
        "                                        replace=True,\n",
        "                                        n_samples=X_train[y_train == 1].shape[0],\n",
        "                                        random_state=123)\n",
        "#\n",
        "# Append the oversampled minority class to training data and related labels\n",
        "#\n",
        "X_balanced = np.vstack((X_train[y_train == 1], X_oversampled))\n",
        "y_balanced = np.hstack((y_train[y_train == 1], y_oversampled))\n",
        "y_balanced\n",
        "cols = X_test.columns.to_list()\n",
        "X_balanced_train = pd.DataFrame(X_balanced, columns=cols)\n",
        "\n",
        "# Create adaboost classifer object\n",
        "abc = AdaBoostClassifier(n_estimators=50,\n",
        "                         learning_rate=1)\n",
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_balanced_train, y_balanced)\n",
        "y_pred = model.predict(X_test)\n",
        "# If we want to use predictions\n",
        "# y_pred is an array of predictions\n",
        "def bestThresshold(y_true,y_pred):\n",
        "    best_thresh = None\n",
        "    best_score = 0\n",
        "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "        score = f1_score(y_true, np.array(y_pred)>thresh)\n",
        "        if score > best_score:\n",
        "            best_thresh = thresh\n",
        "            best_score = score\n",
        "    return best_score , best_thresh\n",
        "\n",
        "bestThresshold(y_test,y_pred)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "# for ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\",roc_auc_score(y_actual, y_pred_proba))\n",
        "\n",
        "# for confusion metric and report\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "plotroc(y_test,y_pred_proba)\n",
        "print(evaluation(y_test, y_pred))\n",
        "# Performance Evaluation Function - Profitability\n",
        "\n",
        "def net_profit(y_test, y_pred_proba,clv=1320*10, acq_cost=200, opp_cost=0):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = confusion_matrix(y_test, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "ada_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "\n",
        "# net profit trend line\n",
        "sns.lineplot(data=ada_profit, x=\"threshold\", y=\"net_profit\")\n",
        "ada_profit[ada_profit['net_profit'] == ada_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvDmyeMHR6PA"
      },
      "source": [
        "Model 5: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-b3g6StSO8_"
      },
      "source": [
        "# Load libraries\n",
        "import imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score,confusion_matrix, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "#data import\n",
        "data = pd.read_csv(\"Insurance_Data_with_Features_final.csv\")\n",
        "data.head()\n",
        "data.drop(columns= ['Unnamed: 0', 'Unnamed: 0.1'],axis=1,inplace=True)\n",
        "data.head()\n",
        "data['Response'].value_counts()\n",
        "y = data['Response']\n",
        "cols = [x for x in data.columns if x not in ['Response', 'id']]\n",
        "X = data[cols]\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())\n",
        "\n",
        "X_oversampled, y_oversampled = resample(X_train[y_train == 0],\n",
        "                                        y_train[y_train == 0],\n",
        "                                        replace=True,\n",
        "                                        n_samples=X_train[y_train == 1].shape[0],\n",
        "                                        random_state=123)\n",
        "#\n",
        "# Append the oversampled minority class to training data and related labels\n",
        "#\n",
        "X_balanced = np.vstack((X_train[y_train == 1], X_oversampled))\n",
        "y_balanced = np.hstack((y_train[y_train == 1], y_oversampled))\n",
        "y_balanced\n",
        "cols = X_test.columns.to_list()\n",
        "X_balanced_train = pd.DataFrame(X_balanced, columns=cols)\n",
        "from sklearn import metrics\n",
        "# If we want to use predictions\n",
        "# y_pred is an array of predictions\n",
        "def bestThresshold(y_true,y_pred):\n",
        "    best_thresh = None\n",
        "    best_score = 0\n",
        "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "        score = metrics.f1_score(y_true, np.array(y_pred)>thresh)\n",
        "        if score > best_score:\n",
        "            best_thresh = thresh\n",
        "            best_score = score\n",
        "    return best_score , best_thresh\n",
        "\n",
        "# for ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\", metrics.roc_auc_score(y_actual, y_pred_proba))\n",
        "\n",
        "# for confusion metric and report\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = metrics.confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(metrics.classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "# Performance Evaluation Function - Profitability\n",
        "\n",
        "def net_profit(y_test, y_pred_proba,clv=1320*5, acq_cost=1000, opp_cost=1320*5):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = metrics.confusion_matrix(y_test, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_RF=RandomForestClassifier(n_estimators=100,random_state=123)\n",
        "model_RF.fit(X_balanced_train, y_balanced)\n",
        "#Predict the response for test dataset\n",
        "y_pred = model_RF.predict(X_test)\n",
        "y_pred_proba = model_RF.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "bestThresshold(y_true=y_test, y_pred=y_pred)\n",
        "plotroc(y_test,y_pred_proba)\n",
        "print(evaluation(y_test, y_pred))\n",
        "RF_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "\n",
        "# net profit trend line\n",
        "sns.lineplot(data=RF_profit, x=\"threshold\", y=\"net_profit\")\n",
        "RF_profit[RF_profit['net_profit'] == RF_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biQk6W-QTadW"
      },
      "source": [
        "Model 6: Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5wL20LqTc77"
      },
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "model_NB = GaussianNB()\n",
        "model_NB.fit(X_balanced_train, y_balanced)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model_NB.predict(X_test)\n",
        "y_pred_proba = model_NB.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "bestThresshold(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "plotroc(y_test,y_pred_proba)\n",
        "print(evaluation(y_test, y_pred))\n",
        "\n",
        "NB_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=NB_profit, x=\"threshold\", y=\"net_profit\")\n",
        "NB_profit[NB_profit['net_profit'] == NB_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-QcpR1Ty1k"
      },
      "source": [
        "Model 7: K- Nearest Neighbor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nhZN7P9T1ht"
      },
      "source": [
        "# KNN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model_KNN =KNeighborsClassifier()\n",
        "model_KNN.fit(X_balanced_train, y_balanced)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model_KNN.predict(X_test)\n",
        "y_pred_proba = model_KNN.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "bestThresshold(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "plotroc(y_test,y_pred_proba)\n",
        "print(evaluation(y_test, y_pred))\n",
        "\n",
        "KNN_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=KNN_profit, x=\"threshold\", y=\"net_profit\")\n",
        "KNN_profit[KNN_profit['net_profit'] == KNN_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okDseocFUQPE"
      },
      "source": [
        "Model 8: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew8ydIIaUSST"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_LogReg = LogisticRegression()\n",
        "model_LogReg.fit(X_balanced_train, y_balanced)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model_LogReg.predict(X_test)\n",
        "y_pred_proba = model_LogReg.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "bestThresshold(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "plotroc(y_test,y_pred_proba)\n",
        "print(evaluation(y_test, y_pred))\n",
        "LogReg_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "\n",
        "# net profit trend line\n",
        "sns.lineplot(data=LogReg_profit, x=\"threshold\", y=\"net_profit\")\n",
        "LogReg_profit[LogReg_profit['net_profit'] == LogReg_profit.net_profit.max()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcLAcGbOPnK"
      },
      "source": [
        "Model 9: Decision Tree with cost complexity pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrfnJJR-OVlh"
      },
      "source": [
        "pip install pycaret\n",
        "\n",
        "#load the libraries\n",
        "import pandas as pd #to manipulate dataframes\n",
        "import numpy as np #to manipulate data series\n",
        "import sklearn #for ML models\n",
        "import pycaret #for initial model selection\n",
        "import seaborn as sn #nice library for visuals\n",
        "import matplotlib.pyplot as plt #to create graphs\n",
        "from sklearn.tree import DecisionTreeClassifier #to build a classification Tree\n",
        "from sklearn.tree import plot_tree #to draw a classification tree\n",
        "from sklearn.model_selection import train_test_split #to split data into training and testing sets\n",
        "from sklearn.model_selection import cross_val_score #for cross validation \n",
        "from sklearn.metrics import confusion_matrix #to create a confusion matrix\n",
        "from sklearn.metrics import plot_confusion_matrix #to plot the confusion matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "#data import\n",
        "data = pd.read_csv(\"/content/gdrive/MyDrive/Machine Learning/Insurance_Data_with_Features_final.csv\")\n",
        "data.head()\n",
        "data.tail()\n",
        "\n",
        "#data check and clean\n",
        "data = data.drop(data.columns[0:3], axis=1)\n",
        "data.head()\n",
        "df1 = data[data['Response'] == 1]\n",
        "df1.head()\n",
        "df2 = data[data['Response'] == 0]\n",
        "df2.head()\n",
        "df2 = df2.sample(frac = .15)\n",
        "df2.head()\n",
        "\n",
        "#combining the positive (df1) and negative (df2) datasets together to create the test and train dataframes\n",
        "frames = [df1, df2]\n",
        "data_sample = pd.concat(frames)\n",
        "data_sample['Response'].unique()\n",
        "#check for the number of observations in the data.\n",
        "len(data_sample)\n",
        "\n",
        "#Splitting the data into two parts: a) data that will be used to make classifications, and b) data that will be used for prediction\n",
        "##Make a new copy of the columns used to make predictions\n",
        "X = data_sample.drop('Response', axis=1).copy()\n",
        "X.head()\n",
        "y = data_sample['Response'].copy()\n",
        "y.head()\n",
        "y.unique()\n",
        "\n",
        "#calculate confusion matrix, net profit, and ROC curve\n",
        "# for confusion metric and report\n",
        "def evaluation(y_actual, y_predict):\n",
        "    confus_matrix = metrics.confusion_matrix(y_actual, y_predict)\n",
        "\n",
        "    sns.heatmap(pd.DataFrame(confus_matrix), annot = True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "    labels = ['Not Respond', 'Respond']\n",
        "    print(metrics.classification_report(y_actual, y_predict, target_names = labels))\n",
        "\n",
        "# Performance Evaluation Function - Profitability\n",
        "\n",
        "def net_profit(y_test, y_pred_proba,clv=1320*5, acq_cost=2000, opp_cost=1320*5):\n",
        "    data = []\n",
        "    cols = ['threshold','net_profit','TP','FP','TN','FN']\n",
        "    for p in np.arange(0, 1, 0.01).tolist():\n",
        "        y_pred_relabel = np.where(y_pred_proba>p,1,0)\n",
        "        cm = metrics.confusion_matrix(y_test, y_pred_relabel)\n",
        "        TN = cm[0,0]\n",
        "        FP = cm[0,1]\n",
        "        FN = cm[1,0]\n",
        "        TP = cm[1,1]\n",
        "        net_profit = TP*(clv-acq_cost) - FP*acq_cost - FN*opp_cost\n",
        "        data.append((p, net_profit, TP, FP,TN,FN))\n",
        "    result = pd.DataFrame(data, columns=cols)\n",
        "    return(result)\n",
        "\n",
        "# for ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "def plotroc(y_actual, y_pred_proba):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_actual, y_pred_proba)\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle = '--') # baseline \n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate', fontsize = 14)\n",
        "    plt.ylabel('True Positive Rate', fontsize = 14)\n",
        "    plt.title('ROC Curve', fontsize = 22)\n",
        "    plt.show()\n",
        "    print(\"ROC AUC Score:\", metrics.roc_auc_score(y_actual, y_pred_proba))\n",
        "\n",
        "#Build a preliminary classification tree\n",
        "## Split the sample data into training and testing sets\n",
        "# Should be a 70/30 split. \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
        "y_train.value_counts()\n",
        "y_test.value_counts()\n",
        "y_test1.value_counts()\n",
        "\n",
        "## Split the complete data into training and testing sets\n",
        "# Should be a 70/30 split. \n",
        "X1 = data.drop('Response', axis=1).copy()\n",
        "y1 = data['Response'].copy()\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, random_state=123)\n",
        "\n",
        "X_train.head()\n",
        "\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\n",
        "#sm = SMOTE(sampling_strategy='minority', random_state=7)\n",
        "\n",
        "# Fit the model to generate the data.\n",
        "#X_train, y_train = sm.fit_sample(X_train, y_train)\n",
        "#oversampled_train = pd.concat([pd.DataFrame(y_train), pd.DataFrame(X_train)], axis=1)\n",
        "#oversampled_train.head()\n",
        "#oversampled_train.tail()\n",
        "#Import the SMOTE-NC\n",
        "#from imblearn.over_sampling import SMOTENC\n",
        "#Create the oversampler. For SMOTE-NC we need to pinpoint the column position where is the categorical features are. In this case, 'IsActiveMember' is positioned in the second column we input [1] as the parameter. If you have more than one categorical columns, just input all the columns position\n",
        "#smotenc = SMOTENC([2,3,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39],random_state = 101)\n",
        "#X_oversample, y_oversample = smotenc.fit_resample(X_train, y_train)\n",
        "\n",
        "## Create a decision tree and fit it to the training data\n",
        "clf_dt = DecisionTreeClassifier(random_state=123)\n",
        "clf_dt = clf_dt.fit(X_train, y_train)\n",
        "\n",
        "##We can now plot the tree\n",
        "plt.figure(figsize=(150, 150))\n",
        "plot_tree(clf_dt,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          class_names=[\"Negative Response\", \"Positive Response\"],\n",
        "          feature_names=X.columns);\n",
        "\n",
        "#plot a confusion matrix. plot_confusion_matrix() will run the the test data down the tree and draw a confusion matrix.\n",
        "plot_confusion_matrix(clf_dt, X_test, y_test, display_labels=['Does not Purchase', 'Purchases'])\n",
        "plt.title('Confusion Matrix of Initial Tree Testing Dataset')\n",
        "#plot a confusion matrix. plot_confusion_matrix() will run the the test data down the tree and draw a confusion matrix.\n",
        "plot_confusion_matrix(clf_dt, X_train, y_train, display_labels=['Does not Purchase', 'Purchases'])\n",
        "plt.title('Confusion Matrix of Initial Tree Training Dataset')\n",
        "\n",
        "#Cost complexity pruning\n",
        "path = clf_dt.cost_complexity_pruning_path(X_train, y_train) #determine values for alpha\n",
        "ccp_alphas = path.ccp_alphas #extract different values for alpha\n",
        "ccp_alphas = ccp_alphas[:-1] #exclude the maximum value for alpha\n",
        "\n",
        "clf_dts = [] #create an array that we will put decision trees into\n",
        "\n",
        "# now create one decision tree per value for alpha and store it in the array\n",
        "for ccp_alpha in ccp_alphas:\n",
        "  clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "  clf_dt.fit(X_train, y_train)\n",
        "  clf_dts.append(clf_dt)\n",
        "\n",
        "train_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\n",
        "test_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_title('Accuracy vs alpha for training and testing sets')\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle='steps-post')\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle='steps-post')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "clf_dt = DecisionTreeClassifier(random_state=123, ccp_alpha=0.0009) # create the tree with ccp_alpha = 0.0009\n",
        "# now use 5-fold cross validation to create 5 different training and testing datasets that are then used to train and test the tree. \n",
        "scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n",
        "df = pd.DataFrame(data={'tree': range(5), 'accuracy': scores})\n",
        "\n",
        "df.plot(x='tree', y='accuracy', marker='o', linestyle='--')\n",
        "# create an array to store the results of each fold during cross validation\n",
        "alpha_loop_values = []\n",
        "# for each candidate value for alpha, we will run 5-fold cross validation. \n",
        "# then we will store the mean and standard deviation of the scores (the accuracy) for each call to  cross_val_score in alpha_loop_values\n",
        "for ccp_alpha in ccp_alphas:\n",
        "  clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "  scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n",
        "  alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n",
        "# now we can draw a graph of the means and standard deviations of the scores\n",
        "alpha_results = pd.DataFrame(alpha_loop_values, \n",
        "                             columns=['alpha', 'mean_accuracy', 'std'])\n",
        "alpha_results.plot(x='alpha', \n",
        "                   y='mean_accuracy',\n",
        "                   yerr='std',\n",
        "                   marker='o',\n",
        "                   linestyle='--')\n",
        "\n",
        "\n",
        "alpha_results[(alpha_results['alpha'] > 0.0001)\n",
        "              &\n",
        "              (alpha_results['alpha'] < 0.001)]\n",
        "\n",
        "#Adjust the values for alpha based on what is above - 0.014 and 0.015 are just place holders\n",
        "ideal_ccp_alpha =  alpha_results[(alpha_results['alpha'] > 0.0001)\n",
        "              &\n",
        "              (alpha_results['alpha'] < 0.001)]['alpha']\n",
        "ideal_ccp_alpha\n",
        "ideal_ccp_alpha = 0.000140\n",
        "\n",
        "#Build and train a new decision tree, only this time use the optimal value for alpha\n",
        "clf_dt_pruned = DecisionTreeClassifier(random_state=42,\n",
        "                                       ccp_alpha=ideal_ccp_alpha)\n",
        "clf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)\n",
        "\n",
        "plot_confusion_matrix(clf_dt_pruned,\n",
        "                      X_test,\n",
        "                      y_test,\n",
        "                      display_labels=[\"Negative Response\", \"Positive response\"])\n",
        "\n",
        "##We can now plot the tree\n",
        "plt.figure(figsize=(40, 40))\n",
        "plot_tree(clf_dt_pruned,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          class_names=[\"Negative Response\", \"Positive Response\"],\n",
        "          feature_names=X.columns);\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "import seaborn as sns #nice library for visuals\n",
        "import matplotlib.pyplot as plt #another library for visuals\n",
        "\n",
        "y_pred = clf_dt_pruned.predict(X_test)\n",
        "plot_roc_curve(clf_dt_pruned, X_test, y_test)\n",
        "print(metrics.roc_auc_score(y_pred, y_test))\n",
        "y_pred_proba = clf_dt_pruned.predict_proba(X_test)[:,1]\n",
        "y_pred_label = np.where(y_pred_proba>0.1,1,0)\n",
        "\n",
        "Decision_Tree_CCP_profit = net_profit(y_test=y_test, y_pred_proba=y_pred_proba, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=Decision_Tree_CCP_profit, x=\"threshold\", y=\"net_profit\")\n",
        "Decision_Tree_CCP_profit[Decision_Tree_CCP_profit['net_profit'] == Decision_Tree_CCP_profit.net_profit.max()]\n",
        "\n",
        "#The test dataset had an accuracy of 65.8% when identifying True Positives versus false positives. \n",
        "MB = 6127/(11795+6127)\n",
        "print((1-MB)*100)\n",
        "clf_dt_pruned_1 = DecisionTreeClassifier(random_state=42,\n",
        "                                       ccp_alpha=ideal_ccp_alpha)\n",
        "clf_dt_pruned_1 = clf_dt_pruned_1.fit(X_train1, y_train1)\n",
        "plot_confusion_matrix(clf_dt_pruned,\n",
        "                      X_test1,\n",
        "                      y_test1,\n",
        "                      display_labels=[\"Negative Response\", \"Positive response\"])\n",
        "print(metrics.roc_auc_score(y_pred, y_test))\n",
        "y_pred_proba_Final = clf_dt_pruned.predict_proba(X_test1)[:,1]\n",
        "y_pred_label_Final = np.where(y_pred_proba_Final>0.1,1,0)\n",
        "Decision_Tree_CCP_profit_Final = net_profit(y_test=y_test1, y_pred_proba=y_pred_proba_Final, clv=1320*5, acq_cost=1000, opp_cost=1320*5)\n",
        "# net profit trend line\n",
        "sns.lineplot(data=Decision_Tree_CCP_profit_Final, x=\"threshold\", y=\"net_profit\")\n",
        "Decision_Tree_CCP_profit_Final[Decision_Tree_CCP_profit_Final['net_profit'] == Decision_Tree_CCP_profit_Final.net_profit.max()]\n",
        "plotroc(y_test,y_pred_proba)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}